{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Order Classification Experiments in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequential_tasks import TemporalOrderExp6aSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the random seed for reproducible results.\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"A simple RNN class.\n",
    "    \n",
    "    In PyTorch, subclassing your model from torch.nn.Module\n",
    "    takes care of low-level concerns for you such as defining\n",
    "    the backward pass and keeping track of network parameters.\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    input_size : int\n",
    "        The number of features in the input tensor.\n",
    "    hidden_size : int\n",
    "        The number of features in the hidden state of the RNN.\n",
    "    output_size : int\n",
    "        The number of classes in the output tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # This just calls the base class constructor.\n",
    "        super().__init__()\n",
    "        # Neural network layers assigned as attributes of a Module subclass\n",
    "        # have their parameters registered for training automatically.\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward pass of the simple RNN.\n",
    "        \n",
    "        Subclasses of torch.nn.Module require the user to override\n",
    "        the forward method to define their computation steps.\n",
    "        \n",
    "        Arguments\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor of shape (batch_size, max_sequence_length, input_size).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : torch.Tensor\n",
    "            The output tensor of shape (batch_size, max_sequence_length, output_size)\n",
    "        \"\"\"\n",
    "        # The RNN also returns its hidden state but we don't use it in this example.\n",
    "        # While the RNN can also take a hidden state as input, the RNN\n",
    "        # gets passed a hidden state initialized with zeros by default.\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.linear(x)\n",
    "        out = F.log_softmax(x, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_gen, criterion, optimizer, device):\n",
    "    \"\"\"Train a model to classify sequences for the temporal ordering problem.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model to train.\n",
    "    train_data_gen : TemporalOrderExp6aSequence\n",
    "        The data generator instance used to produce training data.\n",
    "    criterion : torch.nn.Module\n",
    "        The loss function.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimization algorithm used to updated the network parameters.\n",
    "    device : torch.device\n",
    "        The device to which tensors will be moved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        The number of correctly classified sequences and the loss, respectively.\n",
    "    \"\"\"\n",
    "    # Set the model to training mode. This will turn on layers that would\n",
    "    # otherwise behave differently during evaluation, such as dropout or batch normalization.\n",
    "    model.train()\n",
    "    \n",
    "    # Store the number of sequences that were classified correctly.\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Iterate over every batch of sequences. Note that the length of a data generator\n",
    "    # is defined as the number of batches required to produce a total of roughly 1000\n",
    "    # sequences given a batch size.\n",
    "    for batch_idx in range(len(train_data_gen)):\n",
    "        # For each new batch, clear the gradient buffers of the optimized parameters.\n",
    "        # Otherwise, gradients from the previous batch would be accumulated.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Request a batch of sequences and class labels, convert them into tensors\n",
    "        # of the correct type, and then send them to the appropriate device.\n",
    "        data, target = train_data_gen[batch_idx]\n",
    "        data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "        \n",
    "        # Perform the forward pass of the model.\n",
    "        output = model(data)\n",
    "        \n",
    "        # Although the sequences are padded to a specific length, we noted earlier that they\n",
    "        # actually have variable length. This point becomes relevant for interpreting the model's\n",
    "        # output. We are interested in the output of the model AFTER it has seen an entire sequence,\n",
    "        # which for our problem means just after the model encounters the stop token \"E\". But the\n",
    "        # model produces an output at each step of a sequence, which is why the shape of the output\n",
    "        # is (batch_size, max_sequence_length, output_size).\n",
    "        #\n",
    "        # We want to keep only those outputs which correspond to the end of a sequence. One approach\n",
    "        # is to decode the sequences and then store the length of each sequence in a tensor. We can\n",
    "        # then use this tensor for fancy indexing of the output, after subtracting 1 from it to\n",
    "        # account for tensor indices starting from 0. The first dimension of the output needs to be\n",
    "        # indexed using arange for the indexing to correctly pick out each batch. The final output\n",
    "        # will be of shape (batch_size, output_size).\n",
    "        data_decoded = train_data_gen.decode_x_batch(data.numpy())\n",
    "        sequence_end = torch.tensor([len(sequence) for sequence in data_decoded]) - 1\n",
    "        output = output[torch.arange(data.shape[0]).long(), sequence_end, :]\n",
    "\n",
    "        # Compute the value of the loss for this batch. For loss functions like CrossEntropyLoss,\n",
    "        # the second argument is actually expected to be a tensor of class indices rather than\n",
    "        # one-hot encoded class labels. One approach is to take advantage of the one-hot encoding\n",
    "        # of the target and call argmax along its second dimension to create a tensor of shape\n",
    "        # (batch_size) containing the index of the class label that was hot for each sequence.\n",
    "        target = target.argmax(dim=1)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backpropagation through time in two lines!\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # One way to find the number of correctly classified sequences for each batch is to call\n",
    "        # argmax along the second dimension of the output. This works because the output of the\n",
    "        # RNN is the set of log probabilities for that sequence to belong to one of the classes,\n",
    "        # so the result is a tensor of shape (batch_size) containing the class index with the\n",
    "        # highest log probability for each sequence. We can then check for element-wise equality\n",
    "        # between the predictions and the target and reduce the result to a scalar using sum.\n",
    "        y_pred = output.argmax(dim=1)\n",
    "        num_correct += (y_pred == target).sum().item()\n",
    "\n",
    "    return num_correct, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Testing Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data_gen, criterion, device):\n",
    "    \"\"\"Test a model's classification performance for the temporal ordering problem.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model to evaluate.\n",
    "    test_data_gen : TemporalOrderExp6aSequence\n",
    "        The data generator instance used to produce test data.\n",
    "    criterion : torch.nn.Module\n",
    "        The loss function.\n",
    "    device : torch.device\n",
    "        The device to which tensors will be moved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        The number of correctly classified sequences and the loss, respectively.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode. This will turn off layers that would\n",
    "    # otherwise behave differently during training, such as dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # Store the number of sequences that were classified correctly.\n",
    "    num_correct = 0\n",
    "\n",
    "    # A context manager is used to disable gradient calculations during inference\n",
    "    # to reduce memory usage, as we typically don't need the gradients at this point.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(test_data_gen)):\n",
    "            data, target = test_data_gen[batch_idx]\n",
    "            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            data_decoded = test_data_gen.decode_x_batch(data.numpy())\n",
    "            sequence_end = torch.tensor([len(sequence) for sequence in data_decoded]) - 1\n",
    "            output = output[torch.arange(data.shape[0]).long(), sequence_end, :]\n",
    "\n",
    "            target = target.argmax(dim=1)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            y_pred = output.argmax(dim=1)\n",
    "            num_correct += (y_pred == target).sum().item()\n",
    "\n",
    "    return num_correct, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Putting it All Together\n",
    "\n",
    "Now that we have defined the training and testing loops, our simple RNN is ready for training! Let us combined them into a single function below to streamline the training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=True):\n",
    "    \"\"\"Train a model and monitor its performance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model to evaluate.\n",
    "    train_data_gen : TemporalOrderExp6aSequence\n",
    "        The data generator instance used to produce training data.\n",
    "    test_data_gen : TemporalOrderExp6aSequence\n",
    "        The data generator instance used to produce test data.\n",
    "    criterion : torch.nn.Module\n",
    "        The loss function.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimization algorithm used to updated the network parameters.\n",
    "    max_epochs : int\n",
    "        The maximum number of times model iterates through all of the\n",
    "        training data during training.\n",
    "    verbose : bool, optional\n",
    "        Report the loss and accuracy over the training and test sets for\n",
    "        every epoch. The default is True.\n",
    "    \"\"\"\n",
    "    # Automatically determine the device that PyTorch should use for computation.\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Track the value of the loss function and model accuracy across epochs.\n",
    "    history_train = {'loss': [], 'acc': []}\n",
    "    history_test = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Run the training loop and calculate the accuracy.\n",
    "        # Remember that the length of a data generator is the number of batches,\n",
    "        # so we multiply it by the batch size to recover the total number of sequences.\n",
    "        num_correct, loss = train(model, train_data_gen, criterion, optimizer, device)\n",
    "        accuracy = float(num_correct) / (len(train_data_gen) * train_data_gen.batch_size) * 100\n",
    "        history_train['loss'].append(loss)\n",
    "        history_train['acc'].append(accuracy)\n",
    "        \n",
    "        # Do the same for the testing loop.\n",
    "        num_correct, loss = test(model, test_data_gen, criterion, device)\n",
    "        accuracy = float(num_correct) / (len(test_data_gen) * test_data_gen.batch_size) * 100\n",
    "        history_test['loss'].append(loss)\n",
    "        history_test['acc'].append(accuracy)\n",
    "\n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[Epoch {epoch + 1}/{max_epochs}]'\n",
    "                  f\" loss: {history_train['loss'][-1]:.4f}, acc: {history_train['acc'][-1]:2.2f}%\"\n",
    "                  f\" - test_loss: {history_test['loss'][-1]:.4f}, test_acc: {history_test['acc'][-1]:2.2f}%\")\n",
    "    \n",
    "    # Generate diagnostic plots for the loss and accuracy.\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(9, 4.5))\n",
    "    for ax, metric in zip(axes, ['loss', 'acc']):\n",
    "        ax.plot(history_train[metric])\n",
    "        ax.plot(history_test[metric])\n",
    "        ax.set_xlabel('epoch', fontsize=12)\n",
    "        ax.set_ylabel(metric, fontsize=12)\n",
    "        ax.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and test data generators.\n",
    "difficulty     = TemporalOrderExp6aSequence.DifficultyLevel.EASY\n",
    "batch_size     = 32\n",
    "train_data_gen = TemporalOrderExp6aSequence.data_generator(difficulty, batch_size)\n",
    "test_data_gen  = TemporalOrderExp6aSequence.data_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup the RNN and training settings.\n",
    "input_size  = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model       = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.CrossEntropyLoss()\n",
    "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.02)\n",
    "max_epochs  = 10\n",
    "\n",
    "# Train the model.\n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codas ML",
   "language": "python",
   "name": "codasml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
